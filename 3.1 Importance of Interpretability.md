##### 3.1 해석가능성의 중요성
---

만약 머신러닝 모델이 잘 동작한다면, 그냥 머신러닝 모델을 믿고 왜 이런 결과를 냈는지 무시해도 되지 않을까? "문제는 분류의 정확도와 같이 하나의 수치는 실제 마주하는 대부분의 문제들을 설명하기엔 불완전하다는 것이다." (Doshi-Velez and Kim 2017)

해석가능성이 왜 중요한지에 대해 조금 더 살펴보자. 예측 모델에서는 트레이드오프를 해야한다.: 예를들어 소비자 이탈율 또는 약의 효율성 등 무엇이 에측되었는지를 알고싶은지? 아니면 예측 성능에 대한 해석가능성과 왜 특정한 예측 결과가 나왔는지에 대해 알고 싶은지? 몇몇 상황에서는 왜 이런 결정이 되었는지 신경쓰지 않고 테스트 데이터의 성능이 잘 나왔다는 것만 확인하는 경우가 있다. 다른 몇몇 상황에서는 데이터와 왜 모델이 실패할 것인지에 대해 이해하는 것이 직면한 문제를 이해하는데 도움이 되는 경우도 있다. 몇몇 모델들은 이미 검증되거나 리스크가 적은 환경에서 사용되어 실패하더라도 심각한 결과를 마주하지 않아도 되어서 설명이 필요 없을 수 있다. 해석가능성은 문제 상황(problem formalization)의 불완전성으로부터 필요성이 대두되고, 이는 특정 문제 또는 주어진 문제를 예측을 하는 것만으로는 부족하다는 것을 의미한다. 모델은 어떻게 특정 예측을 하게 되었는지 반드시 설명해야하는데, 그 이유는 올바른 예측은 본래 문제의 일부만 해결하기 때문이다. 올바른 예측은 기존 문제의 일부만 해결할 수 있으므로 왜 이런 예측이 나왔는지에 대한 설명이 필요하다. 뒤에 나오는 여러 이유들은 해석가능성과 설명의 필요성에 대한 수요를 이끌어낸다.(Doshi-Velez and Kim 2017 and Miller 2017)

인간의 호기심과 학습: 인간은 예상하지 못한 일이 발생했을 때 주변 환경에 대한 정보가 업데이트 되는 정신적 모델을 가지고 있다. 이러한 업데이트는 예상치 못한 이벤트에 대한 설명을 찾는 과정에서 발생한다. 예를들어, 한 사람이 예상치 못하게 아프고 왜 아픈지 묻는다. 그는 붉은 베리를 먹을 때 마다 아프다는 것을 학습한다. 그는 붉은 베리는 아픔을 유발하므로 피해야한다는 것을 정신적 모델에 업데이트한다. 불투명한 모델이 연구에 사용되어 모델에 대한 설명 없이 예측 결과만 알려준다면 과학적 발견은 감춰진 채로 유지될 것이다. 왜 특정 결과를 예측했는지 또는 왜 기계가 특정 행동을 하는지에 대해 배우고 호기심을 채우기 위해서는, 해석가능성과 설명은 중요하다. 당연히, 인간은 일어나는 모든 일에 대한 설명이 필요하지는 않다. 대부분의 사람들은 어떻게 컴퓨터가 동작하는지 몰라도 괜찮다. 예상치 못한 이벤트는 궁금증을 유발한다. 예를들어, 왜 컴퓨터가 갑자기 꺼지는지?
