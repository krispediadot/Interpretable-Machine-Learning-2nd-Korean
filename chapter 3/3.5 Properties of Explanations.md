
##### 3.5 설명의 요소
---

우리는 머신러닝 모델의 예측에 대해 설명하고자 한다. 이를 달성하기 위해 설명을 생성하는 몇몇 알고리즘들에 의존한다. 설명은 주로 사람이 이해할 수 있는 방식의 모델 예측의 각 인스턴스 값들과 관련이 있다. 다른 종류의 설명은 데이터 인스턴스들로 구성된다(k-nearest neighbor 모델 등). 예를 들어, 우리는 support vector machine을 사용해서 암의 위험성을 예측할 수 있고 설명을 위해 트리를 생성하는 locala surrogate 모델을 예측의 결과를 설명할 수 있다. 또는 support vector machine 대신 linear regression 모델을 사용할 수도 있다. 선형 모델은 설명 방법이 이미 내제되어있다. 

우리는 설명 방법과 설명의 요소들에 대해 좀 더 자세히 살펴볼 수 있다(Robnik Sikonja and Bohanec, 2018). 이러한 요소들은 얼마나 설명 방법 또는 설명이 좋은지 판단하는데 사용될 수 있다. 이러한 요소들은 어떻게 측정해야 정확한한지가 모호하고 그래서 이를 수식화 하는 것이 하나의 숙제이다.  

설명 방법의 요소

- Expressive Power는 언어 또는 설명의 구조이다. 설명 방법은 IF-THEN 규칙, 의사 결정 트리, 가중 함, 자연어 또는 다른 무언가를 생성할 수 있다. 
- 투명도는 얼마나 많은 설명 방법들이 파라미터와 같은 머신러닝 모델의 내부에 의존하는지를 보여준다. 예를 들어, linear regression과 같이 설명 방법이 intrinsic하게 해석가능한 모델은 투명도가 높다. 입력을 조정해서 나오는 결과를 관찰하는 방법은 투명도가 0이다. 시나리오에 따르면, 다른 레벨의 투명도가 필요할 것이다. 투명도가 높은 것의 이점은 설명하기 위해 더 많은 정보에 의존할 수 있다는 점이다. 투명도가 낮은 것의 이점은 이와 같은 설명 방법은 다른 모델에 이식하기 좋다는 것이다. 
- 이식성은 설명 방법이 다른 머신 러닝 모델에서 적용 될 수 있는 범위를 보여준다. 낮은 투명도를 가진 방법은 머신러닝 모델을 블랙 박스처럼 다루기 때문에 이식성이 높다. surrogate 모델은 이식성이 높은 모델일 것이다. recurrent neural network에만 적용되는 방식은 이식성이 낮다. 
- 알고리즘의 복잡성은 설명을 생성하는 방식의 computational 복잡성을 나타낸다. 이 특성은 설명을 생성하는데 bottlenexk이 발생하면 중요하게 고려할 부분이다. 

설명의 요소

- 정확도: 얼마나 설명이 아직 살펴보지 않은 새로운 데이터에 대해 잘 설명하는지? 높은 정확도는 머신러닝 모델에서 설명이 예측을 위해 사용될 때 중요하다. 낮은 정확도는 머신러닝 모델의 정확도 자체가 낮을때 그리고 설명의 목적이 블래박스 모델이 어떤 일을 하고 있는지는 설명할 때 사용되면 발생할 수 있다. 이러한 경우에는 현실 반영도가 중요하다.
- 현실 반영도: 얼마나 설명이 블랙박스 모델의 예측을 잘 추정하는지? 높은 현실 반영도는 설명에서 중요한 요소중 하나이고, 그 이유는 현실 반영도가 낮은 설명은 머신러닝 모델을 사용하는데 아무 쓸모가 없기 때문이다. 정확도와 현실 반영도는 밀접하게 관계가 있다. 만약 블랙박스 모델이 높은 정확도를 가지고 있고 높은 현실 반영도를 가지고 있다면, 이는 설명이 모델의 일부 데이터에 대해서 잘 추정하고 있거나(local surrogate models) 각각의 데이터 인스턴스에 대해서만 잘 추정하고 있다는(shapley values) 의미이다. 
- 항상성: 얼마나 설명이 동일한 task에 대해 학습하고 비슷한 예측을 나타내는 모델의 차이에 대해 설명하는가? 예를 들어, support vector machine과 linear regression 모델을 동일한 task를 위해 학습시키고 둘 다 비슷한 예측 생성했다. 선택에 따라 설명을 계산했고 두 설명이 얼마나 다른지 분석했다. 이 요소가 tricky 하다는 걸 알게 되었는데, 왜나하면 두 모델이 다른 특성을 사용하지만 비슷한 예측을 하기 때문이다. 이러한 경우에는 높은 항상성으 요구되지 않는데 그 이유는 이때는 설명이 매우 달라야하기 때문이다. 높은 항상성은 모딜에 정말 비슷한 관계가 있을 때 요구된다. 
- 안정성: 얼마나 비슷한 인스턴스에 대해 비슷한 설명을 하는가? 항상성이 모델간의 설명을 비교할 때 안정성은 고정된 모델에서 비슷한 인스턴스에 대해 비교한다. 높은 안정성은 인스턴스의 특성의 작은 변동을 의미하고 대체적으로 설명을 바꾸지 않는 것을 의미한다(그렇지 않다면 작은 변동은 예측의 큰 변화를 일으킨다.). 안정성이 부족하다는 것은 설명 방법에 변동이 크다는 결과일 수도 있다. 다르게 말하면, 


Stability: How similar are the explanations for similar instances? While consistency
compares explanations between models, stability compares explanations between similar
instances for a fixed model. High stability means that slight variations in the features of an
instance do not substantially change the explanation (unless these slight variations also
strongly change the prediction). A lack of stability can be the result of a high variance of
the explanation method. In other words, the explanation method is strongly affected by
slight changes of the feature values of the instance to be explained. A lack of stability can
also be caused by non-deterministic components of the explanation method, such as a
data sampling step, like the local surrogate method uses. High stability is always
desirable.




Comprehensibility: How well do humans understand the explanations? This looks just
like one more property among many, but it is the elephant in the room. Difficult to define
and measure, but extremely important to get right. Many people agree that
comprehensibility depends on the audience. Ideas for measuring comprehensibility
include measuring the size of the explanation (number of features with a non-zero weight
in a linear model, number of decision rules, …) or testing how well people can predict the
behavior of the machine learning model from the explanations. The comprehensibility of
the features used in the explanation should also be considered. A complex transformation
of features might be less comprehensible than the original features.
Certainty: Does the explanation reflect the certainty of the machine learning model?
Many machine learning models only give predictions without a statement about the
models confidence that the prediction is correct. If the model predicts a 4% probability of
cancer for one patient, is it as certain as the 4% probability that another patient, with
different feature values, received? An explanation that includes the model’s certainty is
very useful.
Degree of Importance: How well does the explanation reflect the importance of features
or parts of the explanation? For example, if a decision rule is generated as an explanation
for an individual prediction, is it clear which of the conditions of the rule was the most
important?
Novelty: Does the explanation reflect whether a data instance to be explained comes
from a “new” region far removed from the distribution of training data? In such cases, the
model may be inaccurate and the explanation may be useless. The concept of novelty is
related to the concept of certainty. The higher the novelty, the more likely it is that the
model will have low certainty due to lack of data.
Representativeness: How many instances does an explanation cover? Explanations can
cover the entire model (e.g. interpretation of weights in a linear regression model) or
represent only an individual prediction (e.g. Shapley Values).
