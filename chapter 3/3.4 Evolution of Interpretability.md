
##### 3.4 해석가능성의 평가
---

머신러닝에서의 해석가능성이 무엇인지에 대한 실질적인 합의는 없었다. 이것을 어떻게 측정하는지에 대한 것도 당연히 합의가 없었다. 그러나 이에 대한 몇몇 기초적인 연구는 있고 아래에 구분지어놓은 것처럼 이를 평가하기위해 수식화 하기 위한 시도는 있었다. 

Doshi-Velez and Kim (2017)는 해석가능성을 평가하기 위해 3가지 주요 레벨을 제안했다.

어플리케이션 레벨에서의 평가(실제 task)는 제품에 대한 설명을 놓고 사용자에 의해 테스트 된다. x-ray 안에 속해 있는 머신러닝 요소가 포함된 결함 감지 소프트웨어를 생각해보라. 어플리케이션 레벨에서, 방사선사는 결함 감지 소프트웨어를 직접적으로 평가할 것이다. 이를 위해서는 실험 설정과 결과에 어떻게 접근하는지에 대한 이해가 필요하다. 이러한 평가를 위한 가장 좋은 베이스라인은 평가하는 사람의 설명 능숙도이다. 

사람 레벨에서의 평가(간단한 task)는 어플리케이션 레벨에서의 평가가 단순화된 것이다. 차이점은 실험이 해당 분야의 전문가가 하는 것이 아닌 전문 지식이 많이 없는 일반 사람이 한다는 점이다. 이는 실험을 저렴하게 진행할 수 있도록 하고 테스터를 더 쉽게 찾을 수 있도록 한다. 사용자 마다 다른 설명을 사용자에게 보여줄 것이고 사용자는 그 중 가장 좋은 것을 선택하면 된다. 

함수 레벨의 평가(프록시 task)는 사람이 필요하지 않다. 이러한 작업은 사람 레벨에서의 평가가 이미 이루어진 모델을 사용할 때 가장 유용하다. 예를 들어, 이는 사용자가 의사 결정 트리를 이해하고 있다고 하자. 이러한 경우 설명의 품질은 트리의 깊이일 것이다. 짧은 트리가 더 높은 설명가능성 점수를 받을 것이다. 트리의 예측 성능은 좋은 상태 그대로 두고 제약 조건을 추가하는 것과 더 큰 트리에 비해 크기를 줄이지 않는 것은 이해가 된다. 

다음 장에서는 함수 레벨에서의 각 예측에 대한 평가를 하는 것을 집중적으로 볼 것이다. 평가에 영향이 있는 요소들은 어떤 것들이 있을까?
