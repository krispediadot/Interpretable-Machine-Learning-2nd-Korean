##### 3.1 해석가능성의 중요성
---

만약 머신러닝 모델이 잘 동작한다면, 그냥 머신러닝 모델을 믿고 왜 이런 결과를 냈는지 무시해도 되지 않을까? "문제는 분류의 정확도와 같이 하나의 수치는 실제 마주하는 대부분의 문제들을 설명하기엔 불완전하다는 것이다." (Doshi-Velez and Kim 2017)

해석가능성이 왜 중요한지에 대해 조금 더 살펴보자. 예측 모델에서는 트레이드오프를 해야한다.: 예를들어 소비자 이탈율 또는 약의 효율성 등 무엇이 에측되었는지를 알고싶은지? 아니면 예측 성능에 대한 해석가능성과 왜 특정한 예측 결과가 나왔는지에 대해 알고 싶은지? 몇몇 상황에서는 왜 이런 결정이 되었는지 신경쓰지 않고 테스트 데이터의 성능이 잘 나왔다는 것만 확인하는 경우가 있다. 다른 몇몇 상황에서는 데이터와 왜 모델이 실패할 것인지에 대해 이해하는 것이 직면한 문제를 이해하는데 도움이 되는 경우도 있다. 몇몇 모델들은 이미 검증되거나 리스크가 적은 환경에서 사용되어 실패하더라도 심각한 결과를 마주하지 않아도 되어서 설명이 필요 없을 수 있다. 해석가능성은 문제 상황(problem formalization)의 불완전성으로부터 필요성이 대두되고, 이는 특정 문제 또는 주어진 문제를 예측을 하는 것만으로는 부족하다는 것을 의미한다. 모델은 어떻게 특정 예측을 하게 되었는지 반드시 설명해야하는데, 그 이유는 올바른 예측은 본래 문제의 일부만 해결하기 때문이다. 올바른 예측은 기존 문제의 일부만 해결할 수 있으므로 왜 이런 예측이 나왔는지에 대한 설명이 필요하다. 뒤에 나오는 여러 이유들은 해석가능성과 설명의 필요성에 대한 수요를 이끌어낸다.(Doshi-Velez and Kim 2017 and Miller 2017)

인간의 호기심과 학습: 인간은 예상하지 못한 일이 발생했을 때 주변 환경에 대한 정보가 업데이트 되는 정신적 모델을 가지고 있다. 이러한 업데이트는 예상치 못한 이벤트에 대한 설명을 찾는 과정에서 발생한다. 예를들어, 한 사람이 예상치 못하게 아프고 왜 아픈지 묻는다. 그는 붉은 베리를 먹을 때 마다 아프다는 것을 학습한다. 그는 붉은 베리는 아픔을 유발하므로 피해야한다는 것을 정신적 모델에 업데이트한다. 불투명한 모델이 연구에 사용되어 모델에 대한 설명 없이 예측 결과만 알려준다면 과학적 발견은 감춰진 채로 유지될 것이다. 왜 특정 결과를 예측했는지 또는 왜 기계가 특정 행동을 하는지에 대해 배우고 호기심을 채우기 위해서는, 해석가능성과 설명은 중요하다. 당연히, 인간은 일어나는 모든 일에 대한 설명이 필요하지는 않다. 대부분의 사람들은 어떻게 컴퓨터가 동작하는지 몰라도 괜찮다. 예상치 못한 이벤트는 궁금증을 유발한다. 예를들어, 왜 컴퓨터가 갑자기 꺼지는지?

학습과 밀접하게 관련된 인간의 욕망은 세상에서 새로운 의미를 찾는 것이다. 우리는 지식 구조의 요소들 사이 모순과 불연속성에서 조화를 이루기를 원한다. "이전엔 안 그러던 개가 왜 갑자기 물까?"에 대해 의문을 품을 수 있다. 개의 과거 행동과 물림이라는 달갑지 않은 새로운 경험의 모순이 있다. 수의사의 설명이 개 주인의 모순을 중재한다. "개가 스트레스를 받아서 그래요." 기계의 결정이 인간의 삶에 영향을 크게 미치면 미칠수록 기계의 행동을 설명하는 것은 더욱 중요해진다. 만약 머신러닝 모델이 대출 신청자를 거절한다면, 대출 신청자에게는 예상치 못한 일일 것이다. 그들은 예상했던 것과 실제 사이의 불연속성을 조금의 설명으로 중재할 것이다. 상황에 대한 모든 내용을 설명할 순 없지만, 해당하는 상황이 발생한 주요 원인은 언급해줘야 한다. 다른 예시로는 물건 추천 알고리즘이 있다. 개인적으로 왜 특정 물건이나 영화가 나에게 추천되는 것인지 항상 생각한다. 종종 이유는 명확하다: 최근에 세탁기를 샀기 때문에 며칠 뒤 세탁기 광고를 받게 될 것이란 것을 알고 있다. 겨울 모자가 쇼핑 카트에 들어있다면 장갑을 제안하는 것은 말이 된다. 특정 영화를 추천해주는 이유는 내가 좋아하는 영화를 좋아했던 사람들이 추천된 영화를 좋아했기 때문이다. 점점 인터넷 회사들은 그들의 추천에 대한 설명을 추가하는 추세이다. 가장 좋은 예시는 자주 구매하는 제품의 조합으로 제품을 추천해주는 시스템이다.

많은 과학 분야는 질적 방법에서 양적 방법으로 그리고 머신러닝 방법으로 변화하는 추세이다. 과학의 목적은 지식을 얻는 것인데 많은 문제가 대규모 데이터와 블랙박스 머신러닝 모델로 풀리고 있다. 데이터가 아닌 모델 자체가 지식의 근원이 되고 있다. 해석 가능성은 모델에서 추가적인 지식을 추출할 수 있도록 한다.

머신러닝 모델은 안전한 측정과 테스팅이 필요한 현실세계의 문제를 다룬다. 자율주행 자동차가 딥러닝 시스템을 기반으로 자전거타는 사람을 자동으로 인식하는 것을 상상해보라. 자전거를 타고 가는 사람을 넘어서 가는 것은 위험하므로 오류없이 학습되고 100% 신뢰하는 시스템을 원할 것이다. 설명은 가장 주요하게 학습된 특징이 자전거의 두 개의 바퀴라는 것을 밝혀낼 것이고, 이러한 설명은 가방에 의해 일부 가려진 자전거 바퀴 등 edge case를 생각하는데 도움을 줄 것이다.

기본적으로, 머신러닝 모델은 학습 데이터의 편향을 찾아낸다. 이는 머신러닝 모델이 학습 데이터셋에 충분히 표현되지 않은 그룹을 차별하는 인종차별주의자로 만들 수도 있다. 해석가능성은 머신러닝 모델의 편향을 디버깅하는 도구로 사용할 수 있다. 자동으로 신용 대출 신청자를 승인 또는 거절하는 머신러닝 모델이 그동안 권리를 박탈당해온 소수자를 차별할 수도 있다. 모델의 주된 목적은 최종 빌린 돈을 갚을 사람들에게 대출을 해 주는 것이다. 이 케이스에서 불완전한 문제의 수식은 대출의 기본을 낮추는것 뿐만 아니라 특정 인구를 차별하지 않는 것도 포함되어야한다. 이는 문제의 수식에 추가적인 제약조건이고 머신러닝 모델이 최적화하는 손실함수로 커버될 수 없는 영역이다.

기계와 알고리즘이 일상에 적용되기 위해서는 사회적 수용력을 높이기 위해 해석 가능성이 필요하다. 유명한 실험인 Heider and Simmel(1994) 실험에서 7명의 참가자가 동그라미 모양의 도형이 문을 열고 방(직사각형)으로 들어가는 영상을 보았다. 참가자들은 도형의 움직임을 사람의 움직임처럼 묘사했고 도형에 감정과 개인의 특성을 부여했다. "Doge"라 불리는 나의 진공청소기 같은 로봇은 좋은 예시이다. 만약 로봇이 어딘가에 걸려있다면 나는 "Doge가 계속 청소하고 싶어 하지만 어딘가에 걸려 나에게 도움을 요청하고 있어"라고 생각한다. Doge가 청소를 끝내고 충전하기 위해 제자리로 돌아온다면 나는 "충전하기 위해 원래 위치를 찾고 있다."라고 생각한다. 그리고 나 또한 "Doge는 약간 멍청하지만 귀여워"라며 Doge에게 사람의 특성을 부여한다. 특히 Doge가 집을 청소하면서 화분을 넘어뜨릴 때면 이런 생각을 한다. 기계 또는 알고리즘은 예측이 이 더욱 수영 가능하도록 설명한다. 설명에 관한 다른 챕터들을 보아도, 설명은 사회적 과정이라는 것을 말한다.  

---
설명은 사회적 상호작용을 관리하는데 사용된다. 공유되는 의미를 만듬으로서 설명자는 받아들이는 사람의 행동, 감정 그리고 믿음에 영향을 준다. 기계가 우리와 상호작용 하기 위해서는, 우리의 감정과 믿음을 만드는 것이 필요하다. 기계는 그들의 목표를 이루기 위해서는 우리를 설득해야한다. 나는 로봇 청소기에대한 일부 설명이 없다면 로봇청소기를 충분히 받아들이지 않을 것이다. 예를들면, 청소기에게 "사고"(화장실 카페트에 걸려있는 것)는 단순히 아무말 없이 작동을 멈춘 것이 아니라는 것을 설명하는 것처럼 청소기는 공유된 의미를 생성한다. 흥미롭게도, 기계를 설명하는것(신뢰를 쌓는 것)과 받아들이는 것(예측 또는 행동을 이해는 것) 사이에는 불협화음이 있다. Doge가 왜 그곳에 걸려있었는지에 대해서는 배터리가 거의 없어서, 바퀴중 하나가 제대로 동작하지 않아서 또는 버그가 있어서 장애물이 있어도 로봇이 동일한 장소에 계속 방문하는 것 등으로 설명될 수 있다. 앞서 말한 원인들로 로봇이 한곳에 걸려있을 수 있으나, 이는 어떠한 일이 있었고 이 행동에 대한 설명으로 충분히 가능성 있고 사고에 대한 공유된 의미를 얻는것에 불과하다. 그나저나, Doge는 화장실에 또 걸렸고 Doge를 청소하라고 보내기전에 카페트를 치워야한다. 
